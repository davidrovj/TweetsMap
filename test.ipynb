{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "from credentials import *\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019, 19:37:50) [MSC v.1916 64 bit (AMD64)]\n",
      "3.9.0\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)\n",
    "print(tweepy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authenticated\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Establecer acceso a la API de Twitter.\"\"\"\n",
    "CONSUMER_KEY = \"uvcMjAH4FJglTG8QS9zlpJC01\"\n",
    "CONSUMER_SECRET = 'Ed2c2FhXzpHFagbfTVb3gTnziNTQ3hDk0UDRiiraPfupO5At0s'\n",
    "ACCESS_TOKEN = \"1442475234913775619-hKJkC4Pp3N0ztkvGAue6niVGtra8YZ\"\n",
    "ACCESS_SECRET = \"QqW0LjrlYr1YrkllT1RhMbi4LmUvARaecltG6HTzAobUf\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "print(\"authenticated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tampico_madero = [-97.87777, 18.28519, -96.87777, 24.28519, -97.83623, 18.27228, -96.83623, 24.27228]\n",
    "cdmx =  [-99.12766,19.42847,-98.12766,20.42847,-99.06224,19.35529,-98.06224,20.35529]\n",
    "#cdmx_puebla = [-99.12766,19.42847,-98.12766,20.42847,-99.06224,19.35529,-98.06224,20.35529]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONVERTIR TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[]:\n",
    "\"\"\"Definir la clase que permitirá el uso del Twitter Streaming API.\"\"\"\n",
    "#Twitter Streaming API nos permite descargar mensajes de twitter en tiempo real\n",
    "class listener(tweepy.StreamListener):\n",
    "    \n",
    "    def __init__(self, numero_tweets):\n",
    "        self.received_tweets_counter = 0\n",
    "        self.max_number_tweets = numero_tweets\n",
    "        #Directorio y nombre del archivo de texto donde guardaremos los tweets\n",
    "        self.file = open('C:/Users/jdroj/Downloads/BigData/TweetsMap3.8/tw_cdmxitz.csv', 'a', encoding=\"UTF-8\")\n",
    "        super(listener, self).__init__()\n",
    "    \n",
    "    def on_data(self, data):\n",
    "        if (self.received_tweets_counter < self.max_number_tweets):\n",
    "            \n",
    "            self.received_tweets_counter += 1\n",
    "            #La API de Twitter devuelve datos en formato JSON,\n",
    "            #asi que hay que decodificarlos.\n",
    "            try:\n",
    "                decoded = json.loads(data)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                return True\n",
    "            #No todos los usuarios tienen habilitada la opcion de geolocalizacion\n",
    "            #Por ello hay que dar formato a cuando no este disponible\n",
    "            if decoded.get('geo') is not None:                \n",
    "                location = str(decoded.get('geo').get('coordinates'))\n",
    "            else:\n",
    "                location = '[,]'\n",
    "            \n",
    "            #Extraer los datos que nos interese de los tweets\n",
    "            text = decoded['text'].replace('\\n',' ')\n",
    "            user = '@' + decoded.get('user').get('screen_name')\n",
    "            created = decoded.get('created_at')\n",
    "            \n",
    "            #Escribir los tweets en el archivo de texto\n",
    "            self.file.write(user + \"|\" + location + \"|\" + created + \"|\" + text + \"\\n\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            self.file.close()\n",
    "            print('Done!')\n",
    "            return False\n",
    "\n",
    "    #def on_status(self, status):\n",
    "        #print(status.text)\n",
    "        \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        #Debido a que el uso de la API de twitter tiene un limite diario debemos\n",
    "        #desconectarnos del stream cuando excedamos dicho limite\n",
    "        if status == 420:\n",
    "            print('status code 420: ' + status)\n",
    "            self.file.close()\n",
    "            #Retornando un False en on_error Conseguimos desconectarnos del Stream\n",
    "            return False\n",
    "        self.file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print('Starting...')\n",
    "    #Crear un Stream, estableciendo el número de tweets que se desean guardar\n",
    "    twitterStream = tweepy.Stream(auth, listener(numero_tweets = 200))\n",
    "    #Iniciar un Stream (Puede tomar mucho tiempo en terminar)\n",
    "    #Mas info. en: https://developer.twitter.com/en/docs/tweets/filter-realtime/guides/basic-stream-parameters\n",
    "    twitterStream.filter(locations = cdmx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.io.parsers.readers.TextFileReader at 0x21cdabb39d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_raw = pd.read_table('C:/Users/jdroj/Downloads/BigData/TweetsMap3.8/tw_cdmxitz.csv', header = None, iterator=True)\n",
    "tweets_2 = pd.DataFrame()\n",
    "tweets_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stamp = Timestamp('1/1/2014 16:20', tz='America/Sao_Paulo')\n",
    "#new_stamp = stamp.tz_convert('US/Eastern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18608/1896174641.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#Debido a que el archivo de texto de los tweets es demasiado grande\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#debe de ser procesado por chunks en lugar de cargar todo el archivo en memoria\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets_raw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#1000 filas por chunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'tweets'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'len'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'|'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jdroj\\Downloads\\BigData\\TweetsMap3.8\\env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mget_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1072\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m             \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_currow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1074\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1076\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jdroj\\Downloads\\BigData\\TweetsMap3.8\\env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jdroj\\Downloads\\BigData\\TweetsMap3.8\\env\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m                 \u001b[1;31m# destructive to chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jdroj\\Downloads\\BigData\\TweetsMap3.8\\env\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while 1:\n",
    "    #Debido a que el archivo de texto de los tweets es demasiado grande\n",
    "    #debe de ser procesado por chunks en lugar de cargar todo el archivo en memoria\n",
    "    tweets = tweets_raw.get_chunk(1000) #1000 filas por chunk\n",
    "    tweets.columns = ['tweets']\n",
    "    tweets['len'] = tweets.tweets.apply(lambda x: len(x.split('|')))\n",
    "    tweets[tweets.len < 4] = np.nan\n",
    "    del tweets['len']\n",
    "    tweets = tweets[tweets.tweets.notnull()]\n",
    "    \n",
    "    #Establecer las columnas y valores que tomarán\n",
    "    tweets['user'] = tweets.tweets.apply(lambda x: x.split('|')[0])\n",
    "    tweets['geo'] = tweets.tweets.apply(lambda x: x.split('|')[1])\n",
    "    tweets['timestamp'] = tweets.tweets.apply(lambda x: x.split('|')[2])\n",
    "    tweets['tweet'] = tweets.tweets.apply(lambda x: x.split('|')[3])\n",
    "    tweets['lat'] = tweets.geo.apply(lambda x: x.split(',')[0].replace('[',''))\n",
    "    tweets['lon'] = tweets.geo.apply(lambda x: x.split(',')[1].replace(']',''))\n",
    "    del tweets['tweets']\n",
    "    del tweets['geo']\n",
    "    \n",
    "    #Convertir las latitudes y longitudes de string a float\n",
    "    tweets['lon'] = pd.to_numeric(tweets['lon'], downcast=\"float\")\n",
    "    tweets['lat'] = pd.to_numeric(tweets['lat'], downcast=\"float\")\n",
    "    \n",
    "    #Cambiar la zona horaria de UTC a GMT-5\n",
    "    tweets['timestamp'] = pd.to_datetime(tweets['timestamp'], utc = False)\n",
    "    #tweets = tweets.set_index('timestamp').tz_convert('America/Mexico_City').reset_index()\n",
    "    \n",
    "    #Almacenar los tweets en el dataframe\n",
    "    tweets_2 = tweets_2.append(tweets, ignore_index = True)\n",
    "    \n",
    "    #Guardar los tweets en un archivo CSV\n",
    "    tweets.to_csv('C:/Users/jdroj/Downloads/BigData/TweetsMap3.8/tw_cdmxitz.csv', mode='a', header=False,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lon = -99.99\n",
    "min_lat = 19.30\n",
    "max_lon = -98.0\n",
    "max_lat = 20.50\n",
    "\n",
    "#Descartar las filas con latitud y longitud nulas\n",
    "tweets_2 = tweets_2[(tweets_2.lat.notnull()) & (tweets_2.lon.notnull())]\n",
    "\n",
    "#Filtrar los tweets que estén dentro de la zona que nos interese\n",
    "tweets_2 = tweets_2[(tweets_2.lon >= min_lon) & (tweets_2.lon <= max_lon) &\n",
    "(tweets_2.lat >= min_lat) & (tweets_2.lat <= max_lat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('heatmap-files/tweets_heatmap_cdmxizt','w') as file:\n",
    " file.write(tweets_2[['lat','lon']].to_string(header=False, index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III_GraficarMapa.py\n",
    "\n",
    "Este script permite graficar las coordenadas del archivo (tweets_heatmap_tam_mad) en un \n",
    "mapa, con ayuda del script heatmap.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import osmviz\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw, ImageFont\n",
    "from IPython.display import Image\n",
    "from IPython import get_ipython\n",
    "import numpy as np\n",
    "import imageio\n",
    "from colour import Color\n",
    "import pandas as pd\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching tiles:   0%|          | 0/63 [00:00<?, ?tile/s]\n",
      "Fetching tiles:   2%|▏         | 1/63 [00:01<01:02,  1.00s/tile]\n",
      "Fetching tiles:   3%|▎         | 2/63 [00:01<00:45,  1.35tile/s]\n",
      "Fetching tiles:   5%|▍         | 3/63 [00:02<00:39,  1.53tile/s]\n",
      "Fetching tiles:   6%|▋         | 4/63 [00:02<00:37,  1.56tile/s]\n",
      "Fetching tiles:   8%|▊         | 5/63 [00:03<00:42,  1.35tile/s]\n",
      "Fetching tiles:  10%|▉         | 6/63 [00:04<00:38,  1.49tile/s]\n",
      "Fetching tiles:  11%|█         | 7/63 [00:04<00:34,  1.60tile/s]\n",
      "Fetching tiles:  13%|█▎        | 8/63 [00:05<00:33,  1.67tile/s]\n",
      "Fetching tiles:  14%|█▍        | 9/63 [00:05<00:31,  1.72tile/s]\n",
      "Fetching tiles:  16%|█▌        | 10/63 [00:06<00:27,  1.94tile/s]\n",
      "Fetching tiles:  17%|█▋        | 11/63 [00:06<00:26,  1.96tile/s]\n",
      "Fetching tiles:  19%|█▉        | 12/63 [00:07<00:26,  1.92tile/s]\n",
      "Fetching tiles:  21%|██        | 13/63 [00:07<00:25,  1.99tile/s]\n",
      "Fetching tiles:  22%|██▏       | 14/63 [00:08<00:24,  1.99tile/s]\n",
      "Fetching tiles:  24%|██▍       | 15/63 [00:08<00:26,  1.81tile/s]\n",
      "Fetching tiles:  25%|██▌       | 16/63 [00:09<00:27,  1.71tile/s]\n",
      "Fetching tiles:  27%|██▋       | 17/63 [00:10<00:28,  1.63tile/s]\n",
      "Fetching tiles:  29%|██▊       | 18/63 [00:10<00:28,  1.56tile/s]\n",
      "Fetching tiles:  30%|███       | 19/63 [00:11<00:24,  1.79tile/s]\n",
      "Fetching tiles:  32%|███▏      | 20/63 [00:11<00:21,  2.00tile/s]\n",
      "Fetching tiles:  33%|███▎      | 21/63 [00:12<00:21,  1.95tile/s]\n",
      "Fetching tiles:  35%|███▍      | 22/63 [00:12<00:21,  1.94tile/s]\n",
      "Fetching tiles:  37%|███▋      | 23/63 [00:13<00:25,  1.59tile/s]\n",
      "Fetching tiles:  38%|███▊      | 24/63 [00:14<00:25,  1.52tile/s]\n",
      "Fetching tiles:  40%|███▉      | 25/63 [00:15<00:25,  1.49tile/s]\n",
      "Fetching tiles:  41%|████▏     | 26/63 [00:15<00:26,  1.41tile/s]\n",
      "Fetching tiles:  43%|████▎     | 27/63 [00:16<00:24,  1.44tile/s]\n",
      "Fetching tiles:  44%|████▍     | 28/63 [00:16<00:22,  1.55tile/s]\n",
      "Fetching tiles:  46%|████▌     | 29/63 [00:17<00:20,  1.65tile/s]\n",
      "Fetching tiles:  48%|████▊     | 30/63 [00:18<00:19,  1.73tile/s]\n",
      "Fetching tiles:  49%|████▉     | 31/63 [00:18<00:19,  1.63tile/s]\n",
      "Fetching tiles:  51%|█████     | 32/63 [00:19<00:18,  1.70tile/s]\n",
      "Fetching tiles:  52%|█████▏    | 33/63 [00:19<00:17,  1.76tile/s]\n",
      "Fetching tiles:  54%|█████▍    | 34/63 [00:20<00:16,  1.79tile/s]\n",
      "Fetching tiles:  56%|█████▌    | 35/63 [00:20<00:14,  2.00tile/s]\n",
      "Fetching tiles:  57%|█████▋    | 36/63 [00:21<00:12,  2.20tile/s]\n",
      "Fetching tiles:  59%|█████▊    | 37/63 [00:21<00:11,  2.33tile/s]\n",
      "Fetching tiles:  60%|██████    | 38/63 [00:21<00:11,  2.21tile/s]\n",
      "Fetching tiles:  62%|██████▏   | 39/63 [00:22<00:11,  2.13tile/s]\n",
      "Fetching tiles:  63%|██████▎   | 40/63 [00:22<00:11,  2.08tile/s]\n",
      "Fetching tiles:  65%|██████▌   | 41/63 [00:23<00:10,  2.03tile/s]\n",
      "Fetching tiles:  67%|██████▋   | 42/63 [00:23<00:10,  2.01tile/s]\n",
      "Fetching tiles:  68%|██████▊   | 43/63 [00:24<00:10,  1.98tile/s]\n",
      "Fetching tiles:  70%|██████▉   | 44/63 [00:24<00:09,  1.95tile/s]\n",
      "Fetching tiles:  71%|███████▏  | 45/63 [00:25<00:09,  1.93tile/s]\n",
      "Fetching tiles:  73%|███████▎  | 46/63 [00:25<00:07,  2.15tile/s]\n",
      "Fetching tiles:  75%|███████▍  | 47/63 [00:26<00:07,  2.07tile/s]\n",
      "Fetching tiles:  76%|███████▌  | 48/63 [00:26<00:07,  1.97tile/s]\n",
      "Fetching tiles:  78%|███████▊  | 49/63 [00:27<00:07,  1.98tile/s]\n",
      "Fetching tiles:  79%|███████▉  | 50/63 [00:27<00:06,  1.97tile/s]\n",
      "Fetching tiles:  81%|████████  | 51/63 [00:28<00:05,  2.16tile/s]\n",
      "Fetching tiles:  83%|████████▎ | 52/63 [00:28<00:05,  2.05tile/s]\n",
      "Fetching tiles:  84%|████████▍ | 53/63 [00:29<00:04,  2.02tile/s]\n",
      "Fetching tiles:  86%|████████▌ | 54/63 [00:30<00:04,  1.82tile/s]\n",
      "Fetching tiles:  87%|████████▋ | 55/63 [00:30<00:03,  2.05tile/s]\n",
      "Fetching tiles:  89%|████████▉ | 56/63 [00:30<00:03,  2.21tile/s]\n",
      "Fetching tiles:  90%|█████████ | 57/63 [00:31<00:02,  2.10tile/s]\n",
      "Fetching tiles:  92%|█████████▏| 58/63 [00:31<00:02,  2.05tile/s]\n",
      "Fetching tiles:  94%|█████████▎| 59/63 [00:32<00:01,  2.26tile/s]\n",
      "Fetching tiles:  95%|█████████▌| 60/63 [00:32<00:01,  2.16tile/s]\n",
      "Fetching tiles:  97%|█████████▋| 61/63 [00:33<00:00,  2.09tile/s]\n",
      "Fetching tiles:  98%|█████████▊| 62/63 [00:33<00:00,  1.87tile/s]\n",
      "Fetching tiles: 100%|██████████| 63/63 [00:34<00:00,  1.71tile/s]\n",
      "Fetching tiles: 100%|██████████| 63/63 [00:34<00:00,  1.82tile/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using C:\\Users\\jdroj\\AppData\\Local\\Temp to cache maptiles.\n"
     ]
    }
   ],
   "source": [
    "get_ipython().system('python heatmap.py -o maps/map_tweets_cdmxizt.png --width 1920 -p heatmap-files/tweets_heatmap_cdmxizt -b black -P equirectangular --osm --osm_base http://tile.memomaps.de/tilegen --decay 0.8 -r 10 --zoom 0 --margin 15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = PIL.Image.open('maps/map_tweets_cdmxizt.png')\n",
    "draw = ImageDraw.Draw(im)\n",
    "font = ImageFont.truetype(\"fonts/ProductSans.ttf\", 14)\n",
    "draw.text((1020, 600),\"CDMX, Mex.\", fill = \"black\", font=font)\n",
    "draw.text((1080, 620),\"43 tweets 24/07/20\", fill = \"black\", font=font)\n",
    "im.save('maps/map_tweets_cdmxizt_01.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir un nuevo gradiente de color que sustituya los puntos amarillos que marca heatmap.py por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from int32 to uint8. Range [0, 255]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "hsva_min = Color()\n",
    "hsva_min.hex_l = '#008dcd'\n",
    "\n",
    "hsva_max = Color()\n",
    "hsva_max.hex_l = '#4dccff'\n",
    "\n",
    "color_gradient = list(hsva_max.range_to(hsva_min,256))\n",
    "alpha = np.arange(0,256)[::-1]\n",
    "\n",
    "gradient = []\n",
    "for i, color_point in enumerate(color_gradient):\n",
    "    rgb = list(color_point.get_rgb())\n",
    "    rgb = [int(e * 255) for e in rgb]\n",
    "    rgb.append(alpha[i])\n",
    "    gradient.append([rgb])\n",
    "color_gradient = np.array(gradient)\n",
    "\n",
    "width = 43\n",
    "from copy import deepcopy\n",
    "\n",
    "color_gradient_row = deepcopy(color_gradient)\n",
    "\n",
    "for col in range(width-1):\n",
    "    color_gradient = np.hstack((color_gradient, color_gradient_row))\n",
    "    \n",
    "imageio.imwrite('gradients/gradient_blue.png', color_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using C:\\Users\\jdroj\\AppData\\Local\\Temp to cache maptiles."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching tiles:   0%|          | 0/63 [00:00<?, ?tile/s]\n",
      "Fetching tiles:   2%|▏         | 1/63 [00:00<00:51,  1.21tile/s]\n",
      "Fetching tiles:   3%|▎         | 2/63 [00:01<00:33,  1.84tile/s]\n",
      "Fetching tiles:   5%|▍         | 3/63 [00:01<00:26,  2.24tile/s]\n",
      "Fetching tiles:   6%|▋         | 4/63 [00:01<00:25,  2.35tile/s]\n",
      "Fetching tiles:   8%|▊         | 5/63 [00:02<00:22,  2.57tile/s]\n",
      "Fetching tiles:  10%|▉         | 6/63 [00:02<00:22,  2.51tile/s]\n",
      "Fetching tiles:  11%|█         | 7/63 [00:02<00:19,  2.85tile/s]\n",
      "Fetching tiles:  13%|█▎        | 8/63 [00:03<00:17,  3.16tile/s]\n",
      "Fetching tiles:  14%|█▍        | 9/63 [00:03<00:15,  3.50tile/s]\n",
      "Fetching tiles:  16%|█▌        | 10/63 [00:03<00:20,  2.57tile/s]\n",
      "Fetching tiles:  17%|█▋        | 11/63 [00:04<00:19,  2.73tile/s]\n",
      "Fetching tiles:  19%|█▉        | 12/63 [00:04<00:16,  3.13tile/s]\n",
      "Fetching tiles:  21%|██        | 13/63 [00:04<00:14,  3.50tile/s]\n",
      "Fetching tiles:  22%|██▏       | 14/63 [00:04<00:12,  3.79tile/s]\n",
      "Fetching tiles:  24%|██▍       | 15/63 [00:05<00:11,  4.02tile/s]\n",
      "Fetching tiles:  25%|██▌       | 16/63 [00:05<00:11,  4.15tile/s]\n",
      "Fetching tiles:  27%|██▋       | 17/63 [00:05<00:10,  4.28tile/s]\n",
      "Fetching tiles:  29%|██▊       | 18/63 [00:05<00:10,  4.21tile/s]\n",
      "Fetching tiles:  30%|███       | 19/63 [00:06<00:11,  3.85tile/s]\n",
      "Fetching tiles:  32%|███▏      | 20/63 [00:06<00:11,  3.77tile/s]\n",
      "Fetching tiles:  33%|███▎      | 21/63 [00:06<00:10,  4.01tile/s]\n",
      "Fetching tiles:  35%|███▍      | 22/63 [00:06<00:09,  4.19tile/s]\n",
      "Fetching tiles:  37%|███▋      | 23/63 [00:07<00:09,  4.34tile/s]\n",
      "Fetching tiles:  38%|███▊      | 24/63 [00:07<00:08,  4.40tile/s]\n",
      "Fetching tiles:  40%|███▉      | 25/63 [00:07<00:08,  4.48tile/s]\n",
      "Fetching tiles:  41%|████▏     | 26/63 [00:07<00:08,  4.38tile/s]\n",
      "Fetching tiles:  43%|████▎     | 27/63 [00:08<00:08,  4.03tile/s]\n",
      "Fetching tiles:  44%|████▍     | 28/63 [00:08<00:09,  3.80tile/s]\n",
      "Fetching tiles:  46%|████▌     | 29/63 [00:08<00:09,  3.65tile/s]\n",
      "Fetching tiles:  48%|████▊     | 30/63 [00:08<00:08,  3.88tile/s]\n",
      "Fetching tiles:  49%|████▉     | 31/63 [00:09<00:07,  4.11tile/s]\n",
      "Fetching tiles:  51%|█████     | 32/63 [00:09<00:07,  4.27tile/s]\n",
      "Fetching tiles:  52%|█████▏    | 33/63 [00:09<00:06,  4.44tile/s]\n",
      "Fetching tiles:  54%|█████▍    | 34/63 [00:09<00:06,  4.50tile/s]\n",
      "Fetching tiles:  56%|█████▌    | 35/63 [00:09<00:06,  4.57tile/s]\n",
      "Fetching tiles:  57%|█████▋    | 36/63 [00:10<00:06,  4.45tile/s]\n",
      "Fetching tiles:  59%|█████▊    | 37/63 [00:10<00:06,  4.27tile/s]\n",
      "Fetching tiles:  60%|██████    | 38/63 [00:10<00:06,  3.99tile/s]\n",
      "Fetching tiles:  62%|██████▏   | 39/63 [00:10<00:05,  4.18tile/s]\n",
      "Fetching tiles:  63%|██████▎   | 40/63 [00:11<00:05,  4.26tile/s]\n",
      "Fetching tiles:  65%|██████▌   | 41/63 [00:11<00:05,  4.29tile/s]\n",
      "Fetching tiles:  67%|██████▋   | 42/63 [00:11<00:04,  4.41tile/s]\n",
      "Fetching tiles:  68%|██████▊   | 43/63 [00:11<00:04,  4.50tile/s]\n",
      "Fetching tiles:  70%|██████▉   | 44/63 [00:11<00:04,  4.68tile/s]\n",
      "Fetching tiles:  71%|███████▏  | 45/63 [00:12<00:04,  4.04tile/s]\n",
      "Fetching tiles:  73%|███████▎  | 46/63 [00:12<00:04,  3.94tile/s]\n",
      "Fetching tiles:  75%|███████▍  | 47/63 [00:12<00:04,  3.66tile/s]\n",
      "Fetching tiles:  76%|███████▌  | 48/63 [00:13<00:05,  2.72tile/s]\n",
      "Fetching tiles:  78%|███████▊  | 49/63 [00:13<00:04,  3.08tile/s]\n",
      "Fetching tiles:  79%|███████▉  | 50/63 [00:13<00:03,  3.47tile/s]\n",
      "Fetching tiles:  81%|████████  | 51/63 [00:14<00:03,  3.74tile/s]\n",
      "Fetching tiles:  83%|████████▎ | 52/63 [00:14<00:02,  3.97tile/s]\n",
      "Fetching tiles:  84%|████████▍ | 53/63 [00:14<00:02,  4.20tile/s]\n",
      "Fetching tiles:  86%|████████▌ | 54/63 [00:14<00:02,  3.49tile/s]\n",
      "Fetching tiles:  87%|████████▋ | 55/63 [00:15<00:02,  3.41tile/s]\n",
      "Fetching tiles:  89%|████████▉ | 56/63 [00:15<00:02,  3.32tile/s]\n",
      "Fetching tiles:  90%|█████████ | 57/63 [00:15<00:01,  3.64tile/s]\n",
      "Fetching tiles:  92%|█████████▏| 58/63 [00:15<00:01,  3.90tile/s]\n",
      "Fetching tiles:  94%|█████████▎| 59/63 [00:16<00:00,  4.04tile/s]\n",
      "Fetching tiles:  95%|█████████▌| 60/63 [00:16<00:00,  4.28tile/s]\n",
      "Fetching tiles:  97%|█████████▋| 61/63 [00:16<00:00,  4.34tile/s]\n",
      "Fetching tiles:  98%|█████████▊| 62/63 [00:17<00:00,  3.53tile/s]\n",
      "Fetching tiles: 100%|██████████| 63/63 [00:17<00:00,  2.96tile/s]\n",
      "Fetching tiles: 100%|██████████| 63/63 [00:17<00:00,  3.60tile/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "get_ipython().system('python heatmap.py -G gradients/gradient_blue.png -o maps/map_tweets_cdmxizt_02.png --width 1920 -p heatmap-files/tweets_heatmap_cdmxizt -b black -P equirectangular --osm --osm_base https://basemaps.cartocdn.com/rastertiles/dark_all --decay 0.8 -r 10 --zoom 0 --margin 15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jdroj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "import osmviz\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw, ImageFont\n",
    "from IPython.display import Image\n",
    "from IPython import get_ipython\n",
    "import numpy as np\n",
    "import imageio\n",
    "from colour import Color\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import langid\n",
    "from langdetect import detect\n",
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',1000)\n",
    "try:\n",
    "    general_tweets_corpus_train = pd.read_csv('TASS-Dataset/general-train-tagged.csv', encoding='utf-8')\n",
    "except:\n",
    "    from lxml import objectify\n",
    "    xml = objectify.parse(open('TASS-Dataset/general-train-tagged.xml', encoding=\"UTF-8\"))\n",
    "    root = xml.getroot()\n",
    "    general_tweets_corpus_train = pd.DataFrame(columns=('content', 'polarity', 'agreement'))\n",
    "    tweets = root.getchildren()\n",
    "    \n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content', 'polarity', 'agreement'], \n",
    "                       [tweet.content.text, tweet.sentiments.polarity.value.text,\n",
    "                        tweet.sentiments.polarity.type.text]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        general_tweets_corpus_train = general_tweets_corpus_train.append(row_s)\n",
    "        general_tweets_corpus_train.to_csv('TASS-Dataset/general-train-tagged.csv', index=False, encoding='utf-8')\n",
    "\n",
    "try:\n",
    "    general_tweets_corpus_test = pd.read_csv('TASS-Dataset/general-test-tagged.csv', encoding='utf-8')\n",
    "except:\n",
    "    from lxml import objectify\n",
    "    xml = objectify.parse(open('TASS-Dataset/general-test-tagged.xml', encoding=\"UTF-8\"))\n",
    "    root = xml.getroot()\n",
    "    general_tweets_corpus_test = pd.DataFrame(columns=('content', 'polarity'))\n",
    "    tweets = root.getchildren()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content', 'polarity', 'agreement'], [tweet.content.text, tweet.sentiments.polarity.value.text]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        general_tweets_corpus_test = general_tweets_corpus_test.append(row_s)\n",
    "        general_tweets_corpus_test.to_csv('TASS-Dataset/general-test-tagged.csv', index=False, encoding='utf-8')\n",
    "        \n",
    "try:\n",
    "    stompol_tweets_corpus_train = pd.read_csv('TASS-Dataset/stompol-train-tagged.csv', encoding='utf-8')\n",
    "except:\n",
    "    from lxml import objectify\n",
    "    xml = objectify.parse(open('TASS-Dataset/stompol-train-tagged.xml', encoding = \"UTF-8\"))\n",
    "    root = xml.getroot()\n",
    "    stompol_tweets_corpus_train = pd.DataFrame(columns=('content', 'polarity'))\n",
    "    tweets = root.getchildren()\n",
    "    \n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content', 'polarity', 'agreement'], [' '.join(list(tweet.itertext())), tweet.sentiment.get('polarity')]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        stompol_tweets_corpus_train = stompol_tweets_corpus_train.append(row_s)\n",
    "        stompol_tweets_corpus_train.to_csv('TASS-Dataset/stompol-train-tagged.csv', index=False, encoding='utf-8')\n",
    "\n",
    "try:\n",
    "    stompol_tweets_corpus_test = pd.read_csv('TASS-Dataset/stompol-test-tagged.csv', encoding='utf-8')\n",
    "except:\n",
    "    from lxml import objectify\n",
    "    xml = objectify.parse(open('TASS-Dataset/stompol-test-tagged.xml', encoding = \"UTF-8\"))\n",
    "    root = xml.getroot()\n",
    "    stompol_tweets_corpus_test = pd.DataFrame(columns=('content', 'polarity'))\n",
    "    tweets = root.getchildren()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content', 'polarity', 'agreement'], [' '.join(list(tweet.itertext())), tweet.sentiment.get('polarity')]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        stompol_tweets_corpus_test = stompol_tweets_corpus_test.append(row_s)\n",
    "        stompol_tweets_corpus_test.to_csv('TASS-Dataset/stompol-test-tagged.csv', index=False, encoding='utf-8')\n",
    "\n",
    "try:\n",
    "    social_tweets_corpus_test = pd.read_csv('TASS-Dataset/socialtv-test-tagged.csv', encoding='utf-8')\n",
    "except:\n",
    "    from lxml import objectify\n",
    "    xml = objectify.parse(open('TASS-Dataset/socialtv-test-tagged.xml', encoding = \"UTF-8\"))\n",
    "    root = xml.getroot()\n",
    "    social_tweets_corpus_test = pd.DataFrame(columns=('content', 'polarity'))\n",
    "    tweets = root.getchildren()\n",
    "    \n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content', 'polarity', 'agreement'], [' '.join(list(tweet.itertext())), tweet.sentiment.get('polarity')]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        social_tweets_corpus_test = social_tweets_corpus_test.append(row_s)\n",
    "        social_tweets_corpus_test.to_csv('TASS-Dataset/socialtv-test-tagged.csv', index=False, encoding='utf-8')\n",
    "\n",
    "try:\n",
    "    social_tweets_corpus_train = pd.read_csv('TASS-Dataset/socialtv-train-tagged.csv', encoding='utf-8')\n",
    "except:\n",
    "    from lxml import objectify\n",
    "    xml = objectify.parse(open('TASS-Dataset/socialtv-train-tagged.xml', encoding = \"UTF-8\"))\n",
    "    root = xml.getroot()\n",
    "    social_tweets_corpus_train = pd.DataFrame(columns=('content', 'polarity'))\n",
    "    tweets = root.getchildren()\n",
    "    \n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content', 'polarity', 'agreement'], [' '.join(list(tweet.itertext())), tweet.sentiment.get('polarity')]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        social_tweets_corpus_train = social_tweets_corpus_train.append(row_s)\n",
    "        social_tweets_corpus_train.to_csv('TASS-Dataset/socialtv-train-tagged.csv', index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "try:\n",
    "    general_tweets_corpus_train_mx = pd.read_csv('TASS-Dataset/general-train-tagged-mx.csv', encoding='utf-8')\n",
    "except:\n",
    "    from lxml import objectify\n",
    "    xml = objectify.parse(open('TASS-Dataset/general-train-tagged-mx.xml', encoding=\"UTF-8\"))\n",
    "    #sample tweet object\n",
    "    root = xml.getroot()\n",
    "    general_tweets_corpus_train_mx = pd.DataFrame(columns=('content', 'polarity'))\n",
    "    tweets = root.getchildren()\n",
    "    \n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content', 'polarity', 'agreement'], [tweet.content.text, tweet.sentiment.polarity.value.text]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        general_tweets_corpus_train_mx = general_tweets_corpus_train_mx.append(row_s)\n",
    "        general_tweets_corpus_train_mx.to_csv('TASS-Dataset/general-train-tagged-mx.csv', index=False, encoding='utf-8')\n",
    "\n",
    "try:\n",
    "    general_tweets_corpus_test_mx = pd.read_csv('TASS-Dataset/general-test-tagged-mx.csv', encoding='utf-8')\n",
    "except:\n",
    "    from lxml import objectify\n",
    "    xml = objectify.parse(open('TASS-Dataset/general-test-tagged-mx.xml', encoding=\"UTF-8\"))\n",
    "    root = xml.getroot()\n",
    "    general_tweets_corpus_test_mx = pd.DataFrame(columns=('content', 'polarity'))\n",
    "    tweets = root.getchildren()\n",
    "    \n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content', 'polarity', 'agreement'], [tweet.content.text, tweet.sentiment.polarity.value.text]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        general_tweets_corpus_test_mx = general_tweets_corpus_test_mx.append(row_s)\n",
    "        general_tweets_corpus_test_mx.to_csv('TASS-Dataset/general-test-tagged-mx.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_corpus = pd.concat([\n",
    "    social_tweets_corpus_train,\n",
    "    social_tweets_corpus_test,\n",
    "    stompol_tweets_corpus_test,\n",
    "    stompol_tweets_corpus_train,\n",
    "    general_tweets_corpus_test,\n",
    "    general_tweets_corpus_train,\n",
    "    general_tweets_corpus_test_mx,\n",
    "    general_tweets_corpus_train_mx\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_corpus = tweets_corpus.query('agreement != \"DISAGREEMENT\" and polarity != \"NONE\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir las funciones para la tokenización y stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminar enlaces\n",
    "tweets_corpus = tweets_corpus[-\n",
    "tweets_corpus.content.str.contains('^http.*$')]\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "non_words = list(punctuation)\n",
    "non_words.extend(['¿', '¡'])\n",
    "non_words.extend(map(str,range(10)))\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    # Eliminar caracteres que no sean letras\n",
    "    text = ''.join([c for c in text if c not in non_words])\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # stem\n",
    "    try:\n",
    "        stems = stem_tokens(tokens, stemmer)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        stems = ['']\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparar el modelo que evaluará los tweets.\n",
    "Agregar una nueva columna al data frame corpus (tweets_corpus) en donde las polaridades de los tweets serán representadas de forma binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdroj\\AppData\\Local\\Temp/ipykernel_18608/548591396.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_corpus.polarity_bin[tweets_corpus.polarity.isin(['P', 'P+'])] = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    0.571094\n",
       "0    0.428906\n",
       "Name: polarity_bin, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_corpus = tweets_corpus[tweets_corpus.polarity != 'NEU']\n",
    "tweets_corpus['polarity_bin'] = 0\n",
    "tweets_corpus.polarity_bin[tweets_corpus.polarity.isin(['P', 'P+'])] = 1\n",
    "tweets_corpus.polarity_bin.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = spanish_stopwords\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', LinearSVC()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)), # unigrams or bigrams\n",
    "    'cls__C': (0.2, 0.5, 0.7),\n",
    "    'cls__loss': ('hinge', 'squared_hinge'),\n",
    "    'cls__max_iter': (500, 1000)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_search = sklearn.model_selection.GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='roc_auc')\n",
    "#grid_search.fit(tweets_corpus.content, tweets_corpus.polarity_bin)\n",
    "#grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jdroj\\Downloads\\BigData\\TweetsMap3.8\\env\\lib\\site-packages\\sklearn\\base.py:324: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.22.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\jdroj\\Downloads\\BigData\\TweetsMap3.8\\env\\lib\\site-packages\\sklearn\\base.py:324: UserWarning: Trying to unpickle estimator LinearSVC from version 0.22.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\jdroj\\Downloads\\BigData\\TweetsMap3.8\\env\\lib\\site-packages\\sklearn\\base.py:324: UserWarning: Trying to unpickle estimator Pipeline from version 0.22.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\jdroj\\Downloads\\BigData\\TweetsMap3.8\\env\\lib\\site-packages\\sklearn\\base.py:324: UserWarning: Trying to unpickle estimator GridSearchCV from version 0.22.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "grid_search = joblib.load('grid_search.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jdroj\\Downloads\\BigData\\TweetsMap3.8\\env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9168214861312085"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearSVC(C=.2, loss='squared_hinge',max_iter=1000,multi_class='ovr',\n",
    "                  random_state=None,penalty='l2',tol=0.0001)\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = spanish_stopwords,\n",
    "    min_df = 50,\n",
    "    max_df = 0.9,\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "corpus_data_features = vectorizer.fit_transform(tweets_corpus.content)\n",
    "corpus_data_features_nd = corpus_data_features.toarray()\n",
    "\n",
    "scores = cross_val_score(\n",
    "    model,\n",
    "    corpus_data_features_nd[0:len(tweets_corpus)],\n",
    "    y=tweets_corpus.polarity_bin,\n",
    "    scoring='roc_auc',\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir el archivo CSV con los tweets (tweets_tampico_madero.csv) en un dataframe y seleccionar\n",
    "aquellos que se encuentren dentro del área que nos interesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('C:/Users/jdroj/Downloads/BigData/TweetsMap3.8/tw_cdmxitz.csv', \n",
    "                     names = [\"timestamp\", \"user\", \"tweet\", \"lat\", \"lon\"],encoding='utf-8')\n",
    "\n",
    "\n",
    "tweets = tweets[tweets.tweet.str.len() < 150]\n",
    "\n",
    "tweets.lat = pd.to_numeric(tweets.lat, errors='coerce')\n",
    "tweets.lon = pd.to_numeric(tweets.lon, errors='coerce')\n",
    "\n",
    "tweets = tweets[tweets.lat.notnull()]\n",
    "\n",
    "min_lon = -99.12766\n",
    "min_lat = 19.42847\n",
    "max_lon = -98.12766\n",
    "max_lat = 20.42847\n",
    "\n",
    "tweets = tweets[(tweets.lat.notnull()) & (tweets.lon.notnull())]\n",
    "tweets = tweets[(tweets.lon > min_lon) & (tweets.lon < max_lon) & (tweets.lat > min_lat) & (tweets.lat < max_lat)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langid_safe(tweet):\n",
    " try:\n",
    "    return langid.classify(tweet)[0]\n",
    " except Exception as e:\n",
    "    pass\n",
    " \n",
    "def langdetect_safe(tweet):\n",
    " try:\n",
    "    return detect(tweet)\n",
    " except Exception as e:\n",
    "    pass\n",
    "\n",
    "def textblob_safe(tweet):\n",
    " try:\n",
    "    return textblob.TextBlob(tweet).detect_language()\n",
    " except Exception as e:\n",
    "    pass\n",
    " \n",
    "#Puede tomar un tiempo en terminar\n",
    "tweets['lang_langid'] = tweets.tweet.apply(langid_safe)\n",
    "tweets['lang_langdetect'] = tweets.tweet.apply(langdetect_safe)\n",
    "tweets['lang_textblob'] = tweets.tweet.apply(textblob_safe)\n",
    "tweets['lang_textblob'] = tweets.tweet.apply(textblob_safe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportar el data frame de los tweets con el campo de identificación de idioma a un archivo CSV (tweets_cdmx.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('heatmap-files/tw_cdmxizt_02.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modificar el data frame de los tweets para solo guardar aquellos que estén en idioma español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.query(''' lang_langdetect == 'es' or lang_langid == 'es' or lang_textblob == 'es' ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizar el modelo entrenado para predecir los sentimientos de lo tweets y exportar el data frame resultante a un \n",
    "archivo CSV (tweets_polarity_bin.csv) y a un archivo de texto (tweets_heatmap_polarity_binary) para ser graficado \n",
    "posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jdroj\\Downloads\\BigData\\TweetsMap3.8\\env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "        analyzer = 'word',\n",
    "        tokenizer = tokenize,\n",
    "        lowercase = True,\n",
    "        stop_words = spanish_stopwords,\n",
    "        min_df = 50,\n",
    "        max_df = 0.9,\n",
    "        ngram_range=(1, 1),\n",
    "        max_features=1000\n",
    "    )),\n",
    "    ('cls', LinearSVC(C=.2, loss='squared_hinge', max_iter=1000, multi_class='ovr',\n",
    "        random_state=None,\n",
    "        penalty='l2',\n",
    "        tol=0.0001\n",
    "    )),\n",
    "])\n",
    "\n",
    "pipeline.fit(tweets_corpus.content, tweets_corpus.polarity_bin)\n",
    "\n",
    "tweets['polarity'] = pipeline.predict(tweets.tweet)\n",
    "tweets[['tweet', 'lat', 'lon', 'polarity']].to_csv('heatmap-files/tweets_polarity_bin.csv', encoding='utf-8')\n",
    "\n",
    "with open('heatmap-files/tweets_heatmap_polarity_binary','w') as file:\n",
    "    file.write(tweets[['lat','lon', 'polarity']].to_string(header=False, index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from int32 to uint8. Range [0, 255]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from int32 to uint8. Range [0, 255]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "# Verde = tweets positivos\n",
    "hsva_min = Color()\n",
    "hsva_min.hex_l = '#24b736'\n",
    "hsva_max = Color()\n",
    "hsva_max.hex_l = '#24b736'\n",
    "color_gradient = list(hsva_max.range_to(hsva_min,256))\n",
    "alpha = np.arange(0,256)[::-1]\n",
    "gradient = []\n",
    "\n",
    "for i, color_point in enumerate(color_gradient):\n",
    "    rgb = list(color_point.get_rgb())\n",
    "    rgb = [int(e * 255) for e in rgb]\n",
    "    rgb.append(alpha[i])\n",
    "    gradient.append([rgb])\n",
    "\n",
    "color_gradient = np.array(gradient)\n",
    "\n",
    "width = 43\n",
    "color_gradient_row = deepcopy(color_gradient)\n",
    "\n",
    "for col in range(width-1):\n",
    "    color_gradient = np.hstack((color_gradient, color_gradient_row))\n",
    "\n",
    "imageio.imwrite('gradients/gradient_green.png', color_gradient)\n",
    "\n",
    "# Rojo = tweets negativos\n",
    "hsva_min = Color()\n",
    "hsva_min.hex_l = '#ff3639'\n",
    "\n",
    "hsva_max = Color()\n",
    "hsva_max.hex_l = '#ff3639'\n",
    "\n",
    "color_gradient = list(hsva_max.range_to(hsva_min,256))\n",
    "alpha = np.arange(0,256)[::-1]\n",
    "\n",
    "gradient = []\n",
    "for i, color_point in enumerate(color_gradient):\n",
    "    rgb = list(color_point.get_rgb())\n",
    "    rgb = [int(e * 255) for e in rgb]\n",
    "    rgb.append(alpha[i])\n",
    "    gradient.append([rgb])\n",
    "\n",
    "color_gradient = np.array(gradient)\n",
    "width = 43\n",
    "\n",
    "color_gradient_row = deepcopy(color_gradient)\n",
    "\n",
    "for col in range(width-1):\n",
    "    color_gradient = np.hstack((color_gradient, color_gradient_row))\n",
    "\n",
    "imageio.imwrite('gradients/gradient_red.png', color_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.470146</td>\n",
       "      <td>-99.125560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.457650</td>\n",
       "      <td>-99.069330</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.157000</td>\n",
       "      <td>-98.189760</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.531414</td>\n",
       "      <td>-98.858920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.511335</td>\n",
       "      <td>-99.041310</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19.458115</td>\n",
       "      <td>-99.069190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19.511335</td>\n",
       "      <td>-99.041310</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19.511335</td>\n",
       "      <td>-99.041310</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19.436243</td>\n",
       "      <td>-99.069560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19.693474</td>\n",
       "      <td>-98.847810</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>19.521318</td>\n",
       "      <td>-98.881650</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20.328500</td>\n",
       "      <td>-98.663600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19.733300</td>\n",
       "      <td>-98.966700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20.124910</td>\n",
       "      <td>-98.740170</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20.158140</td>\n",
       "      <td>-98.203230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>19.849394</td>\n",
       "      <td>-98.481445</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1  2\n",
       "0   19.470146 -99.125560  1\n",
       "1   19.457650 -99.069330  1\n",
       "2   20.157000 -98.189760  0\n",
       "3   19.531414 -98.858920  0\n",
       "4   19.511335 -99.041310  1\n",
       "5   19.458115 -99.069190  0\n",
       "6   19.511335 -99.041310  1\n",
       "7   19.511335 -99.041310  1\n",
       "8   19.436243 -99.069560  1\n",
       "9   19.693474 -98.847810  1\n",
       "10  19.521318 -98.881650  0\n",
       "11  20.328500 -98.663600  1\n",
       "12  19.733300 -98.966700  1\n",
       "13  20.124910 -98.740170  1\n",
       "14  20.158140 -98.203230  1\n",
       "15  19.849394 -98.481445  0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_table('heatmap-files/tweets_heatmap_polarity_binary', encoding='utf-8', sep=' ', header=None)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del tweets[tweets.columns[2]]\n",
    "#del tweets[tweets.columns[0]]\n",
    "tweets\n",
    "tweets.columns = ['lat', 'lon', 'polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('heatmap-files/tweets_with_polarity_negative','w') as file:\n",
    "    file.write(tweets[['lat','lon']][tweets.polarity==0].to_string(header=False, index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweets_with_polarity_positive','w') as file:\n",
    "    file.write(tweets[['lat','lon']][tweets.polarity==1].to_string(header=False, index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using C:\\Users\\jdroj\\AppData\\Local\\Temp to cache maptiles.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching tiles:   0%|          | 0/36 [00:00<?, ?tile/s]\n",
      "Fetching tiles:   3%|▎         | 1/36 [00:00<00:08,  3.92tile/s]\n",
      "Fetching tiles:   6%|▌         | 2/36 [00:00<00:07,  4.77tile/s]\n",
      "Fetching tiles:   8%|▊         | 3/36 [00:00<00:06,  5.16tile/s]\n",
      "Fetching tiles:  11%|█         | 4/36 [00:00<00:05,  5.34tile/s]\n",
      "Fetching tiles:  14%|█▍        | 5/36 [00:01<00:07,  4.36tile/s]\n",
      "Fetching tiles:  17%|█▋        | 6/36 [00:01<00:06,  4.68tile/s]\n",
      "Fetching tiles:  19%|█▉        | 7/36 [00:01<00:05,  5.01tile/s]\n",
      "Fetching tiles:  22%|██▏       | 8/36 [00:01<00:05,  5.22tile/s]\n",
      "Fetching tiles:  25%|██▌       | 9/36 [00:01<00:05,  5.31tile/s]\n",
      "Fetching tiles:  28%|██▊       | 10/36 [00:01<00:04,  5.49tile/s]\n",
      "Fetching tiles:  31%|███       | 11/36 [00:02<00:04,  5.26tile/s]\n",
      "Fetching tiles:  33%|███▎      | 12/36 [00:02<00:04,  5.02tile/s]\n",
      "Fetching tiles:  36%|███▌      | 13/36 [00:02<00:04,  5.21tile/s]\n",
      "Fetching tiles:  39%|███▉      | 14/36 [00:02<00:04,  5.37tile/s]\n",
      "Fetching tiles:  42%|████▏     | 15/36 [00:02<00:03,  5.46tile/s]\n",
      "Fetching tiles:  44%|████▍     | 16/36 [00:03<00:03,  5.37tile/s]\n",
      "Fetching tiles:  47%|████▋     | 17/36 [00:03<00:03,  5.38tile/s]\n",
      "Fetching tiles:  50%|█████     | 18/36 [00:03<00:03,  5.50tile/s]\n",
      "Fetching tiles:  53%|█████▎    | 19/36 [00:03<00:03,  5.54tile/s]\n",
      "Fetching tiles:  56%|█████▌    | 20/36 [00:03<00:02,  5.67tile/s]\n",
      "Fetching tiles:  58%|█████▊    | 21/36 [00:03<00:02,  5.80tile/s]\n",
      "Fetching tiles:  61%|██████    | 22/36 [00:04<00:02,  5.61tile/s]\n",
      "Fetching tiles:  64%|██████▍   | 23/36 [00:04<00:02,  5.73tile/s]\n",
      "Fetching tiles:  67%|██████▋   | 24/36 [00:04<00:02,  5.73tile/s]\n",
      "Fetching tiles:  69%|██████▉   | 25/36 [00:04<00:01,  5.74tile/s]\n",
      "Fetching tiles:  72%|███████▏  | 26/36 [00:04<00:01,  5.19tile/s]\n",
      "Fetching tiles:  75%|███████▌  | 27/36 [00:05<00:01,  5.42tile/s]\n",
      "Fetching tiles:  78%|███████▊  | 28/36 [00:05<00:01,  5.58tile/s]\n",
      "Fetching tiles:  81%|████████  | 29/36 [00:05<00:01,  5.61tile/s]\n",
      "Fetching tiles:  83%|████████▎ | 30/36 [00:05<00:01,  5.70tile/s]\n",
      "Fetching tiles:  86%|████████▌ | 31/36 [00:05<00:00,  5.75tile/s]\n",
      "Fetching tiles:  89%|████████▉ | 32/36 [00:05<00:00,  5.63tile/s]\n",
      "Fetching tiles:  92%|█████████▏| 33/36 [00:06<00:00,  5.72tile/s]\n",
      "Fetching tiles:  94%|█████████▍| 34/36 [00:06<00:00,  5.69tile/s]\n",
      "Fetching tiles:  97%|█████████▋| 35/36 [00:06<00:00,  5.70tile/s]\n",
      "Fetching tiles: 100%|██████████| 36/36 [00:06<00:00,  5.74tile/s]\n",
      "Fetching tiles: 100%|██████████| 36/36 [00:06<00:00,  5.42tile/s]\n"
     ]
    }
   ],
   "source": [
    "get_ipython().system('python heatmap.py -o maps/cdmxizt_tweets_polarity_negative.png --width 1920 -p heatmap-files/tweets_with_polarity_negative -P equirectangular --osm --osm_base https://basemaps.cartocdn.com/rastertiles/light_all --decay 0.8 -r 10 --zoom 0 --margin 15 -G gradients/gradient_red.png --layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using C:\\Users\\jdroj\\AppData\\Local\\Temp to cache maptiles.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching tiles:   0%|          | 0/18 [00:00<?, ?tile/s]\n",
      "Fetching tiles: 100%|██████████| 18/18 [00:00<00:00, 488.44tile/s]\n"
     ]
    }
   ],
   "source": [
    "get_ipython().system('python heatmap.py -o maps/cdmxizt_tweets_polarity_positive.png --width 1920 -p heatmap-files/tweets_with_polarity_positive -P equirectangular --osm --osm_base https://basemaps.cartocdn.com/rastertiles/light_all --decay 0.8 -r 10 --zoom 0 --margin 15 -G gradients/gradient_green.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = PIL.Image.open('maps/cdmxizt_tweets_polarity_positive.png')\n",
    "foreground = PIL.Image.open('maps/cdmxizt_tweets_polarity_negative.png')\n",
    "\n",
    "background.paste(foreground, (0, 0), foreground)\n",
    "draw = ImageDraw.Draw(background)\n",
    "font = ImageFont.truetype(\"fonts/ProductSans.ttf\", 14)\n",
    "\n",
    "draw.text((1020, 600),\"CDMX, Mex.\", fill = \"black\", font=font)\n",
    "draw.text((1020, 620),\"Tweets positivos y negativos\", fill = \"black\", font=font)\n",
    "\n",
    "background.save('maps/cdmxizt_tweets_polarity.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "945e58ad8f9ac57da3f5e23423768d7db4d570607e4532c49de7038272e59413"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
